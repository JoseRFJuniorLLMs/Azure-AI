![[semantic_entropy.png]]


ğƒğğ­ğğœğ­ğ¢ğ§ğ  ğ¡ğšğ¥ğ¥ğ®ğœğ¢ğ§ğšğ­ğ¢ğ¨ğ§ğ¬ ğ¢ğ§ ğ¥ğšğ«ğ ğ ğ¥ğšğ§ğ ğ®ğšğ ğ ğ¦ğ¨ğğğ¥ğ¬ ğ®ğ¬ğ¢ğ§ğ  ğ¬ğğ¦ğšğ§ğ­ğ¢ğœ ğğ§ğ­ğ«ğ¨ğ©ğ²: The paper discusses the effectiveness of semantic entropy in detecting confabulations, which are a significant type of error in model generations. It compares semantic entropy with other methods, highlighting that while some methods can identify various errors, semantic entropy is superior in detecting confabulations specifically. The study emphasizes the importance of context in evaluating model responses, noting that fixed reference answers may not adequately capture the flexibility needed in conversational AI. Overall, the findings suggest that semantic entropy provides a reliable measure for assessing the correctness of model outputs in various contexts.
#### Como a SemÃ¢ntica Entropia Ã© Usada na PrÃ¡tica?

1. **Medir ConsistÃªncia**: Analisar se as respostas do modelo mantÃªm consistÃªncia semÃ¢ntica em relaÃ§Ã£o Ã s perguntas ou informaÃ§Ãµes fornecidas.
2. **Detectar Incertezas**: Identificar respostas com alta entropia semÃ¢ntica, que podem indicar respostas fabricadas ou menos confiÃ¡veis.
3. **Ajuste de Modelos**: Refine os modelos para minimizar a geraÃ§Ã£o de respostas com alta entropia, melhorando a precisÃ£o e confiabilidade.

Para demonstrar a ideia de entropia semÃ¢ntica em um exemplo em Python, podemos usar tÃ©cnicas de Processamento de Linguagem Natural (NLP) para comparar a semÃ¢ntica de diferentes respostas geradas por um modelo de linguagem, como o GPT, em relaÃ§Ã£o a uma pergunta ou contexto dado. 

Neste exemplo, utilizaremos a similaridade de cosseno entre embeddings de sentenÃ§as para avaliar a consistÃªncia semÃ¢ntica. Vamos calcular a similaridade entre a resposta gerada e uma referÃªncia esperada, e usar isso como uma forma de "entropia semÃ¢ntica".

Para isso, utilizaremos a biblioteca `transformers` da Hugging Face e a `sentence-transformers` para calcular as embeddings de sentenÃ§as.

Primeiro, instale as bibliotecas necessÃ¡rias:

```bash
pip install transformers sentence-transformers
```

Agora, crie um script Python para calcular a entropia semÃ¢ntica:

```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

# Carrega o modelo para gerar embeddings das sentenÃ§as
model = SentenceTransformer('all-MiniLM-L6-v2')

# FunÃ§Ã£o para calcular a entropia semÃ¢ntica (similaridade)
def semantic_entropy(reference, response):
    # Calcula as embeddings para as sentenÃ§as de referÃªncia e resposta
    ref_embedding = model.encode(reference, convert_to_tensor=True)
    res_embedding = model.encode(response, convert_to_tensor=True)

    # Calcula a similaridade de cosseno entre as embeddings
    similarity = util.cos_sim(ref_embedding, res_embedding)

    # Converte a similaridade para um valor escalar
    similarity_score = similarity.item()

    # Calcular a entropia semÃ¢ntica como a diferenÃ§a da similaridade
    entropy = 1 - similarity_score

    return entropy

# Exemplos de perguntas e respostas
reference_answer = "O sol Ã© uma estrela que fica no centro do Sistema Solar."
response_1 = "O sol Ã© uma estrela central em nosso sistema."
response_2 = "Os gatos sÃ£o animais domÃ©sticos comuns."
response_3 = "O sistema solar Ã© composto por planetas que orbitam o sol."

# Calcula a entropia semÃ¢ntica para as respostas
entropy_1 = semantic_entropy(reference_answer, response_1)
entropy_2 = semantic_entropy(reference_answer, response_2)
entropy_3 = semantic_entropy(reference_answer, response_3)

# Mostra os resultados
print(f"Entropia semÃ¢ntica para resposta 1: {entropy_1:.4f}")
print(f"Entropia semÃ¢ntica para resposta 2: {entropy_2:.4f}")
print(f"Entropia semÃ¢ntica para resposta 3: {entropy_3:.4f}")
```

### ExplicaÃ§Ã£o do CÃ³digo:
- **Modelo de Embeddings:** Usamos `SentenceTransformer` para gerar embeddings das sentenÃ§as. Essas embeddings sÃ£o vetores numÃ©ricos que capturam o significado semÃ¢ntico das sentenÃ§as.
- **Similaridade de Cosseno:** Calculamos a similaridade de cosseno entre as embeddings da resposta e da referÃªncia. Valores prÃ³ximos de 1 indicam alta similaridade (baixa entropia semÃ¢ntica), enquanto valores prÃ³ximos de 0 indicam baixa similaridade (alta entropia semÃ¢ntica).
- **Entropia SemÃ¢ntica:** Definimos a entropia semÃ¢ntica como \( 1 - \text{similaridade} \). Assim, quanto maior for a entropia, menor a similaridade entre a resposta e a referÃªncia.

### Resultados:
Ao rodar o script, vocÃª verÃ¡ que respostas mais prÃ³ximas da referÃªncia terÃ£o menor entropia semÃ¢ntica, enquanto respostas menos relevantes ou incorretas terÃ£o entropia maior.

Esse exemplo Ã© uma forma simples de demonstrar como a entropia semÃ¢ntica pode ser usada para avaliar a qualidade e a consistÃªncia das respostas geradas por um modelo de linguagem.